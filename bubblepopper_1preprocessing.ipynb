{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean article collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy_utils import database_exists, create_database\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import newspaper\n",
    "from datetime import datetime\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open (\"bubble_popper_postgres.txt\",\"r\") as myfile:\n",
    "    lines = [line.replace(\"\\n\",\"\") for line in myfile.readlines()] \n",
    "db, us, pw = 'bubble_popper', lines[0], lines[1]                     \n",
    "engine = create_engine('postgresql://%s:%s@localhost:5432/%s'%(us,pw,db))\n",
    "connstr = \"dbname='%s' user='%s' host='localhost' password='%s'\"%(db,us,pw)\n",
    "conn = None; conn = psycopg2.connect(connstr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save article information in a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query = \"\"\"SELECT * FROM pub_scores\"\"\"\n",
    "pub_scores = pd.read_sql(query,conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns = ['publication','source','heard','trust','distrust','content','title','url']\n",
    "articles = pd.DataFrame(columns=columns)\n",
    "\n",
    "for handle in pub_scores['twitter']:\n",
    "    \n",
    "    print(str(datetime.now()),handle)\n",
    "    \n",
    "    articleList = pickle.load(open('pub_text_'+handle+'.pkl','rb'))\n",
    "    content = [article.text for article in articleList]\n",
    "    title = [article.title for article in articleList]\n",
    "    url = [article.url for article in articleList]\n",
    "    \n",
    "    publication = np.repeat(pub_scores['Source'][pub_scores['twitter']==handle],len(content))\n",
    "    source = np.repeat(pub_scores['source'][pub_scores['twitter']==handle],len(content))\n",
    "    heard = np.repeat(pub_scores['heard'][pub_scores['twitter']==handle],len(content))\n",
    "    trust = np.repeat(pub_scores['trust'][pub_scores['twitter']==handle],len(content))\n",
    "    distrust = np.repeat(pub_scores['distrust'][pub_scores['twitter']==handle],len(content))\n",
    "    \n",
    "    temp = pd.DataFrame({'publication':publication,\n",
    "                         'source':source,\n",
    "                         'heard':heard,\n",
    "                         'trust':trust,\n",
    "                         'distrust':distrust,\n",
    "                         'content':content,\n",
    "                         'title':title,\n",
    "                         'url':url})\n",
    "    \n",
    "    articles = articles.append(temp,ignore_index=True)\n",
    "\n",
    "pickle.dump(articles,open('pub_articles.pkl','wb'))\n",
    "articles.to_sql('pub_articles',engine,if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove short (usually advertisements), Guardian (British news), Stack of Stuff (list of links), and duplicate articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "short_text = []\n",
    "for i,article in enumerate(pub_articles['content'].values.tolist()):\n",
    "    if len(article)<=0:\n",
    "        short_text.append(i)\n",
    "\n",
    "guardian_text = []\n",
    "for i,publication in enumerate(pub_articles['publication'].values.tolist()):\n",
    "    if publication == 'Guardian':\n",
    "        guardian_text.append(i)\n",
    "\n",
    "stack_text = [i for i in range(0,len(pub_articles)) if 'Stack of Stuff' in pub_articles['title'].iloc[i]]\n",
    "\n",
    "drop_text = short_text + guardian_text + stack_text\n",
    "drop_text = list(set(drop_text))\n",
    "\n",
    "articles = pub_articles.drop(pub_articles.index[drop_text])\n",
    "articles = articles.drop_duplicates('content')\n",
    "\n",
    "pickle.dump(articles,open('pub_articles_trimmed.pkl','wb'))\n",
    "articles.to_sql('pub_articles_clean',engine,if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean article content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim import corpora, models\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove special characters, tokenize and lemmatize the articles, and remove stop and miscellaneous words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_set = articles['content'].values.tolist()\n",
    "\n",
    "doc_set = [doc.replace(\"\\n\",\" \") for doc in doc_set]\n",
    "doc_set = [doc.replace(\"\\'\",\"\") for doc in doc_set]\n",
    "\n",
    "doc_set = [gensim.utils.simple_preprocess(doc) for doc in doc_set]\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "doc_set = [[wordnet_lemmatizer.lemmatize(word) for word in doc] for doc in doc_set]\n",
    "doc_set = [[wordnet_lemmatizer.lemmatize(word,pos='v') for word in doc] for doc in doc_set]\n",
    "\n",
    "en_stop = get_stop_words('en')\n",
    "letters = [\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\",\"j\",\"k\",\"l\",\"m\",\"n\",\"o\",\"p\",\"q\",\"r\",\"s\",\"t\",\"u\",\"v\",\"w\",\"x\",\"y\",\"z\"]\n",
    "other = [\"wa\",\"ha\",\"one\",\"two\",\"id\",\"re\",\"http\",\"com\",\"mr\",\"image\",\"photo\",\"caption\",\"don\",\"sen\",\"pic\",\"co\",\n",
    "         \"source\",\"watch\",\"play\",\"duration\",\"video\",\"momentjs\",\"getty\",\"images\",\"newsletter\"]\n",
    "doc_set = [[word for word in doc if not word in (en_stop+letters+other)] for doc in doc_set]\n",
    "\n",
    "pickle.dump(doc_set,open('pub_articles_cleaned_super.pkl','wb'))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:projects]",
   "language": "python",
   "name": "conda-env-projects-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
